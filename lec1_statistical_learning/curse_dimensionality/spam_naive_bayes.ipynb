{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n",
      "(4601, 57) (4601,)\n"
     ]
    }
   ],
   "source": [
    "orig_data = np.loadtxt('spambase.data', delimiter=',')\n",
    "print(orig_data.shape)\n",
    "X = orig_data[:, :-1]\n",
    "Y = np.ravel(orig_data[:, -1:])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **spambase** data set contains features which charecterize an email, and the classification of those emails as spam or not. There are 57 input variables, and two possible classifications. More information [can be found here](https://archive.ics.uci.edu/ml/datasets/spambase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3450, 57) (1151, 57)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, stratify=Y)\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838401390095569\n"
     ]
    }
   ],
   "source": [
    "def get_test_score(X_train_set, X_test_set):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train_set, Y_train)\n",
    "    return gnb.score(X_test_set, Y_test)\n",
    "\n",
    "print(get_test_score(X_train, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two cells divide the dataset into training set and test set, and then we train a **Naive Bayes Classifier** to classify the spam. In the above example, we are using all of the features in the original dataset, and get a model with an accuracy of about 81%.\n",
    "\n",
    "Your exercise is to train and find test score with data which has fewer features. A simple way to select fewer features is to select the first k features out of the 57. Your final output should be two lists - one called `dims` which contains the dimensions of the data set, and another called `test_scores` which has the accuracy for a data set with the correspoding number of dimensions.\n",
    "\n",
    "Uncomment the lines below and execute the cell to see a graph which shows dimensions vs accuracy. Initially you should see accuracy improve as number of features increase. If we get hit by the **curse of dimensionality**, we will however see the accuracy drop as the number of features swells further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(dims, test_scores)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
